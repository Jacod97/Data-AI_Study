{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 샘플 코드 체험해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[  101, 72337, 72654, 10911, 10155, 11178, 15541, 13028,   112,   172,\n",
      "         11850,   119,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 탐색 1. Tokenizer\n",
    "# Tokenizer -> ?? 텍스트를 넣으면 그 텍스트를 어떤 숫자 tensor로 바꿔주는 역할\n",
    "print(type(encoded_input))\n",
    "print(encoded_input.keys())\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print(encoded_input[\"token_type_ids\"])\n",
    "print(encoded_input[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색 2. model에 넣어줘야 하는 값\n",
    "# model(**encoded_input)\n",
    "# 입력값이 딕셔너리여야 하며 그 키는 input_ids, token_type_ids, attention_mask\n",
    "# 즉, 모델에 대한 입력값은 toknizer의 결과 출력값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 탐색 3. 모델 마지막 출력 바꿔야 한다.\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2524, -0.5321,  0.4496,  ...,  1.1428, -0.6238, -0.0745],\n",
      "         [ 0.5374,  0.0701,  0.4880,  ...,  1.0800, -0.5560, -0.5564],\n",
      "         [ 0.4686, -0.3286,  0.4782,  ...,  1.1656, -0.7389, -0.3920],\n",
      "         ...,\n",
      "         [ 0.4822, -1.0822,  0.9026,  ...,  1.8029, -1.1343, -0.1034],\n",
      "         [ 0.1283, -0.6550,  0.3723,  ...,  0.7817, -0.9173, -0.0401],\n",
      "         [ 0.1645, -0.5239,  0.6638,  ...,  0.7964, -0.7326, -0.2642]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.3479, -0.0466,  0.4564, -0.2464, -0.0563,  0.5865,  0.4807,  0.2475,\n",
      "         -0.5663,  0.4260,  0.0136, -0.3356, -0.3281,  0.0391,  0.3874, -0.4077,\n",
      "          0.8111,  0.2171,  0.4523, -0.5184, -0.9998, -0.5465, -0.1472, -0.5278,\n",
      "         -0.5858,  0.3390, -0.4160,  0.3991,  0.4012, -0.4298,  0.1618, -0.9998,\n",
      "          0.7481,  0.7902,  0.3325, -0.4037,  0.1348,  0.2374,  0.2554, -0.1603,\n",
      "         -0.3249,  0.0911, -0.4920,  0.0966, -0.0886, -0.4324, -0.3835,  0.3972,\n",
      "         -0.5554,  0.2169,  0.1148,  0.3260,  0.4943,  0.4352,  0.3529,  0.2036,\n",
      "          0.3909,  0.1889,  0.5300, -0.4648, -0.0521,  0.5751,  0.2526, -0.0612,\n",
      "         -0.2219, -0.4394,  0.0356, -0.3328,  0.5685, -0.3036, -0.2594, -0.4530,\n",
      "         -0.2713,  0.1009,  0.2301, -0.3336,  0.5517,  0.4013,  0.1831, -0.3138,\n",
      "         -0.5992, -0.5676, -0.5336,  0.3356, -0.2990,  0.5137,  0.2741, -0.5048,\n",
      "          0.2117, -0.0015,  0.3309,  0.6298, -0.2695,  0.2346, -0.3509, -0.4789,\n",
      "         -0.8909, -0.3698, -0.4985, -0.5266, -0.4290,  0.1720, -0.2786, -0.3678,\n",
      "         -0.1528, -0.5241,  0.3222,  0.3790, -0.3511,  0.4393,  0.3300, -0.5506,\n",
      "         -0.2684,  0.2293, -0.4590,  0.9924, -0.5686,  0.5286,  0.0708, -0.2181,\n",
      "         -0.6942,  0.9998,  0.1417, -0.2129,  0.2732,  0.3654, -0.6268,  0.2804,\n",
      "          0.4681,  0.4806,  0.4627, -0.2392, -0.3762, -0.4162, -0.9208, -0.3348,\n",
      "         -0.5620,  0.2746, -0.5168, -0.4194,  0.2401,  0.3975,  0.2670, -0.1696,\n",
      "         -0.3278, -0.0907,  0.5337, -0.3277,  0.9998,  0.6998, -0.3795, -0.0534,\n",
      "          0.7170, -0.7487, -0.3792, -0.3239, -0.3306, -0.7024,  0.3410,  0.3298,\n",
      "          0.3016, -0.3909, -0.5636, -0.3942,  0.2903, -0.7446, -0.3552,  0.5690,\n",
      "          0.4293,  0.4615, -0.2927,  0.5457,  0.4178, -0.2542, -0.3220,  0.5368,\n",
      "          0.3799, -0.2040, -0.4458, -0.1065,  0.3341, -0.3019, -0.6492,  0.1437,\n",
      "         -0.2294, -0.5867,  0.2758, -0.3071, -0.0754,  0.2141, -0.5020,  0.3125,\n",
      "         -0.5100,  0.4330,  0.5638,  0.2154, -0.4454,  0.3568,  0.5825,  0.5002,\n",
      "          0.4783,  0.1944,  0.3854,  0.3343, -0.4531, -0.8202,  0.5794,  0.3621,\n",
      "          0.4884, -0.2074, -0.5437, -0.5644,  0.7088,  0.4359, -0.3276,  0.5428,\n",
      "          0.4145, -0.4658, -0.3687,  0.3062, -0.1896, -0.5597, -0.5004, -0.4566,\n",
      "         -0.1157,  0.4746,  0.2691,  0.1165,  0.1806, -0.2733, -0.2113, -0.4532,\n",
      "          0.2552,  0.2581, -0.1887,  0.9240, -0.3420,  0.1755, -0.5977, -0.2486,\n",
      "          0.3691, -0.2093,  0.3388,  0.9882,  0.2615, -0.4248,  0.1601,  0.2186,\n",
      "          0.2783, -0.3623,  0.1934, -0.7431,  0.8718,  0.5192,  0.3146, -0.9998,\n",
      "          0.3282,  0.1694,  0.2693,  0.3528,  0.3718,  0.2386,  0.3505,  0.9561,\n",
      "         -0.6181, -0.5637, -0.6078, -0.3900, -0.6018, -0.3526, -0.4228, -0.3809,\n",
      "         -0.3002, -0.2034, -0.2592,  0.3707,  0.3969, -0.9981,  0.9230,  0.2382,\n",
      "         -0.2377, -0.0136,  0.4744, -0.9998,  0.4307, -0.2486, -0.4819,  0.4280,\n",
      "         -0.6735, -0.3429,  0.2829,  0.2688,  0.4592,  0.3448,  0.1427,  0.5622,\n",
      "         -0.1548,  0.0800,  0.2396, -0.2551,  0.7024, -0.0637,  0.2483,  0.4902,\n",
      "         -0.0604,  0.4824, -0.4325,  0.5296,  0.5423,  0.1549,  0.2383, -0.3534,\n",
      "          0.4392, -0.8413,  0.4954, -0.0289, -0.1962, -0.0201,  0.3012, -0.2925,\n",
      "         -0.2265,  0.2662, -0.6172,  0.9998,  0.4016, -0.4316, -0.6277,  0.5666,\n",
      "          0.7398, -0.3263, -0.8886, -0.2034,  0.6460,  0.4737,  0.2391,  0.3137,\n",
      "          0.2394,  0.2663, -0.1722, -0.2366, -0.0762, -0.5351,  0.4793, -0.2669,\n",
      "         -0.3655,  0.4261, -0.4167, -0.3533, -0.9110,  0.5237,  0.4789,  0.2668,\n",
      "          0.2215,  0.2964, -0.3993,  0.7121,  0.6148, -0.3470, -0.2579, -0.3197,\n",
      "         -0.3302,  0.0703, -0.3236, -0.3757,  0.3013, -0.7673,  0.2004, -0.1767,\n",
      "         -0.3236, -0.4728,  0.5218, -0.9998, -0.1971,  0.2426, -0.3627,  0.5291,\n",
      "         -0.4952, -0.1579,  0.1851,  0.3242,  0.0453,  0.4090, -0.4824,  0.3272,\n",
      "         -0.2402,  0.3627,  0.8409,  0.7796,  0.2317, -0.4103,  0.3347, -0.5352,\n",
      "         -0.1233,  0.5097,  0.4353, -0.1352,  0.3606,  0.4346,  0.3564, -0.3667,\n",
      "          0.4498, -0.2681, -0.3671,  0.3424,  0.1276, -0.2811, -0.5382,  0.2924,\n",
      "         -0.5711,  0.4475,  0.3389,  0.5766,  0.3297,  0.4118, -0.3788, -0.3290,\n",
      "         -0.1866, -0.1894, -0.5205, -0.5004, -0.3591,  0.9998,  0.4138,  0.4436,\n",
      "         -0.5978,  0.4752,  0.2903, -0.3527,  0.3584,  0.5775,  0.2543, -0.0140,\n",
      "          0.2847,  0.4611,  0.3658,  0.5154,  0.5331,  0.6123, -0.4101,  0.7353,\n",
      "         -0.3032, -0.5571, -0.9984,  0.3908,  0.6632, -0.4190, -0.8705,  0.3718,\n",
      "         -0.3502,  0.2786, -0.4557,  0.0131,  0.3753, -0.3060,  0.5765, -0.2056,\n",
      "          0.9997, -0.2336,  0.3872,  0.4641,  0.3707, -0.2764, -0.4731, -0.4289,\n",
      "          0.4056, -0.4280,  0.1517, -0.9720,  0.4526,  0.0424,  0.3430, -0.2254,\n",
      "          0.5788, -0.5043,  0.4951, -0.1597, -0.4305, -0.3821,  0.4350, -0.5796,\n",
      "          0.5389, -0.3024,  0.1203, -0.4169,  0.3182, -0.1258,  0.5588, -0.2346,\n",
      "          0.0680, -0.3068, -0.3093, -0.4831,  0.1124, -0.3321,  0.9998, -0.1196,\n",
      "          0.5176, -0.4022,  0.4818, -0.4652,  0.7185,  0.7791, -0.2603,  0.4564,\n",
      "          0.4134, -0.7492,  0.4928, -0.1487, -0.8899, -0.0349,  0.9843,  0.2287,\n",
      "          0.2933,  0.7056,  0.4445,  0.5416, -0.3607,  0.2753,  0.9175,  0.2576,\n",
      "          0.2043,  0.3301,  0.1690, -0.2744, -0.4724,  0.9998,  0.9997, -0.0657,\n",
      "          0.3945, -0.3565, -0.3675, -0.3862,  0.2491,  0.1906,  0.3696, -0.2029,\n",
      "          0.0978, -0.4928, -0.2283, -0.2088, -0.3244, -0.3628,  0.2238, -0.5318,\n",
      "          0.6601,  0.4634,  0.3248,  0.5380,  0.4754,  0.3531, -0.2502, -0.4468,\n",
      "          0.5027, -0.4210, -0.2843, -0.3626,  0.2704, -0.9997, -0.3237, -0.3378,\n",
      "         -0.3674,  0.6854,  0.3904,  0.1491, -0.3761, -0.2340, -0.4994,  0.3760,\n",
      "          0.0038,  0.3809, -0.3853, -0.3365,  0.5422, -0.7120,  0.2767, -0.4304,\n",
      "         -0.4116, -0.6568, -0.4123, -0.3864,  0.4490, -0.3626, -0.3944,  0.5106,\n",
      "          0.4320,  0.3441, -0.4078,  0.3311, -0.2481,  0.0281,  0.5284,  0.3057,\n",
      "          0.3534, -0.4360, -0.4805, -0.3101, -0.4968, -0.2919,  0.1183, -0.3756,\n",
      "          0.3986, -0.3174,  0.3155, -0.3968,  0.0747,  0.4007,  0.5872, -0.2769,\n",
      "          0.6463,  0.5545, -0.2109,  0.6229,  0.3648, -0.3702, -0.4417,  0.9998,\n",
      "          0.5258,  0.2709, -0.0111, -0.1960,  0.3316,  0.3600,  0.6899, -0.4168,\n",
      "          0.8816, -0.4170,  0.3142,  0.5387,  0.5203,  0.0401,  0.4016,  0.3202,\n",
      "          0.8566,  0.4476,  0.5010,  0.5407,  0.1573,  0.5883,  0.4044,  0.3473,\n",
      "          0.4646,  0.4544, -0.4164,  0.4577, -0.3750, -0.2294, -0.0772, -0.1680,\n",
      "         -0.3458,  0.0120, -0.2115, -0.2273, -0.1055,  0.5045, -0.3115,  0.1304,\n",
      "         -0.1694, -0.3133,  0.6474, -0.6750,  0.4520, -0.1844,  0.1244, -0.8716,\n",
      "          0.4054, -0.2088, -0.5018, -0.3058, -0.6826,  0.3779,  0.4047, -0.4849,\n",
      "          0.4524, -0.1988,  0.3167, -0.3646, -0.3537,  0.2657, -0.9998,  0.2529,\n",
      "          0.3900, -0.4465,  0.3725,  0.0813,  0.3133,  0.4863, -0.5000, -0.5793,\n",
      "         -0.3430,  0.4808, -0.3672,  0.1632,  0.4605, -0.5725, -0.2863,  0.1940,\n",
      "         -0.3025,  0.3865,  0.5675, -0.3394,  0.5261, -0.4529,  0.1571, -0.4972,\n",
      "          0.2650, -0.5463, -0.2356,  0.4190, -0.4152, -0.6399, -0.0933,  0.3661,\n",
      "         -0.2407,  0.2780,  0.4938, -0.1811,  0.4346, -0.4499,  0.5624, -0.3171,\n",
      "          0.3187, -0.8939, -0.3868, -0.6404, -0.1275,  0.3002,  0.6523,  0.1678,\n",
      "          0.4010, -0.2698,  0.0510, -0.1523,  0.3995,  0.4114, -0.2815,  0.0711,\n",
      "         -0.3346,  0.2971, -0.5793,  0.0084, -0.9974, -0.4989,  0.2654,  0.5173,\n",
      "          0.4936, -0.3613, -0.2462, -0.4643, -0.2649,  0.2476,  0.3840,  0.3994,\n",
      "          0.4215,  0.4807, -0.2922, -0.2126,  0.8593, -0.3262,  0.2572,  0.4530,\n",
      "          0.5275,  0.9433,  0.3779,  0.3931,  0.3170, -0.4342,  0.5104,  0.4348]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...      4\n",
       "1  내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...      5\n",
       "2  검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...      1\n",
       "3  배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...      2\n",
       "4  최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...      3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./배달의민족댓글2.csv\", index_col=0).dropna().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  label\n",
       "0  배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...      4      1\n",
       "1  내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...      5      1\n",
       "2  검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...      1      0\n",
       "3  배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...      2      0\n",
       "4  최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...      3      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3이상이면 긍정 1, 그 외에는 0\n",
    "import numpy as np\n",
    "\n",
    "data[\"label\"] = np.where(data[\"score\"] >=3, 1, 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다 한가지 건의사항이 있다면 최신순별점...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요 분명 이 가게에 시킨 기억...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>검색 화면에서 전체배달포장 탭 중 배달 탭을 스크롤 내리면서 볼 때 아래로 스크롤하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 04000원 이런식으로 된 가게가 가장 위로 올라옵니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다 배민 어플 실행시 업데...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  label  \\\n",
       "0  배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...      4      1   \n",
       "1  내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...      5      1   \n",
       "2  검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...      1      0   \n",
       "3  배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...      2      0   \n",
       "4  최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...      3      1   \n",
       "\n",
       "                                              review  \n",
       "0  배달의민족 주문시 리뷰를 자주 참고하는 편입니다 한가지 건의사항이 있다면 최신순별점...  \n",
       "1  내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요 분명 이 가게에 시킨 기억...  \n",
       "2  검색 화면에서 전체배달포장 탭 중 배달 탭을 스크롤 내리면서 볼 때 아래로 스크롤하...  \n",
       "3  배달팁 낮은 순으로 정렬하면 04000원 이런식으로 된 가게가 가장 위로 올라옵니다...  \n",
       "4  최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다 배민 어플 실행시 업데...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "data[\"review\"] = data[\"text\"].apply(lambda x: re.sub(\"[^0-9a-zA-Z가-힣\\s+]\", \"\", x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 데이터 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX: (800,), TrainY: (800,)\n",
      "TestX: (200,), TestY: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    data[\"review\"],\n",
    "    data[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TrainX: {trainX.shape}, TrainY: {trainY.shape}\")\n",
    "print(f\"TestX: {testX.shape}, TestY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX: (720,), TrainY: (720,)\n",
      "ValX: (80,), ValY: (80,)\n",
      "TestX: (200,), TestY: (200,)\n"
     ]
    }
   ],
   "source": [
    "trainX, valX, trainY, valY = train_test_split(\n",
    "    trainX, trainY, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TrainX: {trainX.shape}, TrainY: {trainY.shape}\")\n",
    "print(f\"ValX: {valX.shape}, ValY: {valY.shape}\")\n",
    "print(f\"TestX: {testX.shape}, TestY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940    주문이 누락됬으면 누락됬다고 알려줘야죠 배달 예정 시간이 지나고 30분이나 지나서 ...\n",
       "547     잘되면 감사하고 더 많은 서비스를 줘야지 고객들 호구로보다가 코로나랑 배달회사랑 ...\n",
       "900    항상 잘 사용하고있습니다 배민에서 오는 알림 내역을 다시 볼 수 있는 목록도 있었으...\n",
       "603    배달팁이 기본 4000원이 넘어갑니다 장난하나요 7000원인 곳도 있어요 물가 올라...\n",
       "348    전에 먹었던 메뉴를 그대로 재주문 할 수 있는 기능이 있으면 좋겠어요 혹시 있나 찾...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 학습데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset Class를 만들어야 한다.\n",
    "# __len__ : 데이터의 개수를 반환한다.\n",
    "# __getitem__ : 학습 데이터 1개 딕셔너리를 반환한다.\n",
    "## tokenizer()의 출력(딕셔너리) + label키를 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트1 - 샘플코드 그대로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "torch.Size([74])\n",
      "torch.Size([74])\n",
      "torch.Size([74])\n",
      "torch.Size([42])\n",
      "torch.Size([42])\n",
      "torch.Size([42])\n",
      "torch.Size([65])\n",
      "torch.Size([65])\n",
      "torch.Size([65])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer)\n",
    "print(len(train_dataset))\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][\"input_ids\"].shape)\n",
    "print(train_dataset[0][\"attention_mask\"].shape)\n",
    "print(train_dataset[0][\"token_type_ids\"].shape)\n",
    "print(train_dataset[1][\"input_ids\"].shape)\n",
    "print(train_dataset[1][\"attention_mask\"].shape)\n",
    "print(train_dataset[1][\"token_type_ids\"].shape)\n",
    "print(train_dataset[2][\"input_ids\"].shape)\n",
    "print(train_dataset[2][\"attention_mask\"].shape)\n",
    "print(train_dataset[2][\"token_type_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트 2 - flatten이 없을 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            max_length = 128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"],\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "tensor([[   101,   9689,  25934,  10739,    100,    100,   9524,  26737, 119221,\n",
      "          21711, 119217,   9330,  89851,   9576,  16605,   9485,  96618,   9706,\n",
      "          16439,  11664,  10244,  37712,  43739,   9706,  16439,  12424,   9258,\n",
      "          34907,  10530,   9665,  18227,  35506,  25503,   9524,  14153, 118800,\n",
      "          77884,  48549,   9995, 118992, 118965,  11018,   8898,  16605,  12092,\n",
      "           9357, 119199,  12453,  48549,  62849,  15303,   9580,  46520,  12030,\n",
      "         119219,   9524, 119118,  41850,   9435,  35465,  43739,  80956,   9641,\n",
      "          10739,  90587,   9668,  34907,   9536,  10622,   9521,   9511,  14153,\n",
      "            100,    102,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "tensor([[   101,   9654, 118800,  14867,   8848,  12945,  12453,   9074,  25685,\n",
      "           9425, 104021,  11513,   9695,  21711,  12508,   8888, 118617,  27023,\n",
      "           9985,  17196,  11261,  80001,  11287,   9812,  11261,  16439,  62200,\n",
      "           9330,  89851,  14863,  12945,  62200,  38401,   9929, 107693, 119424,\n",
      "          77884,  48549,   8956, 119214,  11903,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "tensor([[   101,   9959,  14871,   9654,   9405,  24974,  12453, 119192, 119081,\n",
      "          48345,   9330,  36553,  11489,   9580,  11018,   9524,  67527,   8996,\n",
      "          23160,  10622,  25805,   9359,   9460,  13767,   9284,  31398,  12092,\n",
      "           9647, 119138,  80046,   9685, 118632, 119081,  48345,   9524,  67527,\n",
      "          10739,   9706,  69592,  12508,  14867,   9074,  66982,   9359,   9460,\n",
      "           9555,  12965,  12424,   9290,   9358,   9524,  61250,  10739,   9303,\n",
      "         119147,  11018,  12508,   9524,   9328,  33768,  10739,   9555,  77884,\n",
      "          48549,    102,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][\"input_ids\"])\n",
    "# print(train_dataset[0][\"attention_mask\"].shape)\n",
    "# print(train_dataset[0][\"token_type_ids\"].shape)\n",
    "print(train_dataset[1][\"input_ids\"])\n",
    "# print(train_dataset[1][\"attention_mask\"].shape)\n",
    "# print(train_dataset[1][\"token_type_ids\"].shape)\n",
    "print(train_dataset[2][\"input_ids\"])\n",
    "# print(train_dataset[2][\"attention_mask\"].shape)\n",
    "# print(train_dataset[2][\"token_type_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for x in train_loader:\n",
    "    print(x[\"input_ids\"].shape)\n",
    "    print(x[\"attention_mask\"].shape)\n",
    "    print(x[\"token_type_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⭐ (확정)테스트 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            max_length = self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "tensor([   101,   9689,  25934,  10739,    100,    100,   9524,  26737, 119221,\n",
      "         21711, 119217,   9330,  89851,   9576,  16605,   9485,  96618,   9706,\n",
      "         16439,  11664,  10244,  37712,  43739,   9706,  16439,  12424,   9258,\n",
      "         34907,  10530,   9665,  18227,  35506,  25503,   9524,  14153, 118800,\n",
      "         77884,  48549,   9995, 118992, 118965,  11018,   8898,  16605,  12092,\n",
      "          9357, 119199,  12453,  48549,  62849,  15303,   9580,  46520,  12030,\n",
      "        119219,   9524, 119118,  41850,   9435,  35465,  43739,  80956,   9641,\n",
      "         10739,  90587,   9668,  34907,   9536,  10622,   9521,   9511,  14153,\n",
      "           100,    102,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer, max_length=128)\n",
    "print(len(train_dataset))\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][\"input_ids\"])\n",
    "print(train_dataset[0][\"attention_mask\"])\n",
    "print(train_dataset[0][\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for x in train_loader:\n",
    "    print(x[\"input_ids\"].shape)\n",
    "    print(x[\"attention_mask\"].shape)\n",
    "    print(x[\"token_type_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최종 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "max_length = 128 \n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(valX, valY, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(testX, testY, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 불러오기 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "epochs = 5\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 스케줄러 설정\n",
    "# 워밍업 단계(학습률을 선형적으로 증가) / 학습 단계(학습률을 선형적으로 감소)\n",
    "total_steps = len(train_loader) * epochs \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_history = {\"train\": [], \"val\": []}\n",
    "best_loss_val = float('inf')\n",
    "patience = 5\n",
    "patience_cnt = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #### Train #### \n",
    "    model.train()\n",
    "\n",
    "    loss_train = 0.0\n",
    "    for batch in train_loader:\n",
    "        # GPU 보내기\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # 학습과정\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = labels\n",
    "        )\n",
    "        loss = outputs.loss \n",
    "        loss.backward()\n",
    "        # 기울기 폭주(exploration)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # loss 저장\n",
    "        loss_train += loss.item() * batch_size \n",
    "    \n",
    "    loss_history[\"train\"].append(loss_train / len(train_dataset))\n",
    "\n",
    "    #### Validation ####\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0.0 \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # GPU 보내기\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # 예측\n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                labels = labels\n",
    "            )\n",
    "            loss = outputs.loss \n",
    "\n",
    "            # loss 저장 \n",
    "            loss_val += loss.item() * batch_size\n",
    "\n",
    "        loss_history[\"val\"].append(loss_val / len(val_dataset))\n",
    "    \n",
    "    #### Early Stopping #### \n",
    "    if loss_val < best_loss_val:\n",
    "        best_loss_val = loss_val \n",
    "        torch.save(model.state_dict(), \"best_bert.pth\")\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt == patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break \n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss_train / len(train_dataset)} Validation Loss: {loss_val / len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구조 가져오기\n",
    "from transformers import BertForSequenceClassification \n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "model.load_state_dict(torch.load(\"best_bert.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5207462197542191 Accuracy: 0.7699999809265137\n"
     ]
    }
   ],
   "source": [
    "#### test ###\n",
    "model.eval()\n",
    "\n",
    "loss_test = 0.0 \n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # GPU 보내기\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # 예측\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss \n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct += (labels == preds).sum()\n",
    "\n",
    "        # loss 저장 \n",
    "        loss_test += loss.item() * batch_size\n",
    "\n",
    "print(f\"Test Loss: {loss_test / len(test_dataset)} Accuracy: {correct / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 새로운 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 가져오기\n",
    "from transformers import BertForSequenceClassification \n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 입히기\n",
    "import torch \n",
    "model.load_state_dict(torch.load(\"best_bert.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 128\n",
    "\n",
    "text = \"배달도 늦고 맛도 없어요\"\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    text, \n",
    "    max_length = max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# print(encoded_input)\n",
    "encoded_input = encoded_input.to(device)\n",
    "output = model(**encoded_input)\n",
    "print(output.logits.argmax(dim=1).cpu().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
