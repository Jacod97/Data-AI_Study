{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf11bac",
   "metadata": {},
   "source": [
    "### 동적 크롤링(Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def create_driver():\n",
    "    \"\"\"Selenium ChromeDriver를 생성하는 함수\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def scroll_to_bottom(driver, max_attempts=30):\n",
    "    \"\"\"페이지 끝까지 스크롤을 내리는 함수\"\"\"\n",
    "    scroll_heights = set()\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height in scroll_heights:\n",
    "            break\n",
    "        scroll_heights.add(new_height)\n",
    "        attempts += 1\n",
    "\n",
    "def crawl_naver_blog(query):\n",
    "    \"\"\"네이버 블로그 검색 결과를 크롤링하여 CSV로 저장하는 함수\"\"\"\n",
    "    # 검색어를 URL에 맞게 인코딩\n",
    "    from urllib.parse import quote\n",
    "    encoded_query = quote(query)\n",
    "\n",
    "    url = f\"https://search.naver.com/search.naver?sm=tab_hty.top&ssc=tab.blog.all&query={encoded_query}\"\n",
    "\n",
    "    driver = create_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # 스크롤 두 번\n",
    "    scroll_to_bottom(driver)\n",
    "    time.sleep(2)\n",
    "    scroll_to_bottom(driver)\n",
    "\n",
    "    # 페이지 HTML 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 블로그 글 정보 수집\n",
    "    blogs = []\n",
    "    for link in soup.select(\"a.title_link\"):\n",
    "        title = link.get_text(strip=True)\n",
    "        href = link.get(\"href\")\n",
    "        blogs.append({\"Title\": title, \"URL\": href})\n",
    "\n",
    "    # 데이터프레임 생성 및 저장\n",
    "    df = pd.DataFrame(blogs)\n",
    "    filename = \"naver_blog_posts.csv\"\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"{len(blogs)}개의 블로그 글이 '{filename}'에 저장되었습니다.\")\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881590a4",
   "metadata": {},
   "source": [
    "### 정적 크롤링(requests + BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def naver_developer(search, site, max_results=500):\n",
    "    \"\"\"네이버 API를 사용하여 데이터를 검색하고 'title'과 'link'만 Pandas DataFrame으로 반환\"\"\"\n",
    "    \n",
    "    encText = urllib.parse.quote(search)\n",
    "    client_id = os.getenv(\"NAVER_CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"NAVER_CLIENT_SECRET\")\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"NAVER_CLIENT_ID 또는 NAVER_CLIENT_SECRET이 설정되지 않았습니다.\")\n",
    "\n",
    "    all_results = [] \n",
    "    display = 100 \n",
    "    \n",
    "    for start in range(1, max_results + 1, display):  \n",
    "        url = f\"https://openapi.naver.com/v1/search/{site}.json?query={encText}&display={display}&start={start}&sort=date\"\n",
    "        print(f\"Fetching: {url}\")  \n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "\n",
    "        try:\n",
    "            response = urllib.request.urlopen(request)\n",
    "            rescode = response.getcode()\n",
    "\n",
    "            if rescode == 200:\n",
    "                response_body = response.read()\n",
    "                response_data = response_body.decode('utf-8')\n",
    "                response_dict = json.loads(response_data)\n",
    "                \n",
    "                items = response_dict.get(\"items\", [])\n",
    "                \n",
    "                # title과 link만 저장\n",
    "                filtered_items = [{\"title\": item[\"title\"], \"link\": item[\"link\"]} for item in items]\n",
    "                all_results.extend(filtered_items)  \n",
    "\n",
    "                if len(items) < display:  # 마지막 페이지면 종료\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error Code: {rescode}\")\n",
    "                break\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "            break\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"URL Error: {e.reason}\")\n",
    "            break\n",
    "\n",
    "    # Pandas DataFrame으로 변환\n",
    "    csv_data = pd.DataFrame(all_results)\n",
    "\n",
    "    return csv_data\n",
    "\n",
    "def parsing(df, site):\n",
    "    \"\"\" `BeautifulSoup`을 사용하여 HTML 태그 제거 후 기존 데이터프레임에 반영하고 저장 \"\"\"\n",
    "\n",
    "    def clean_html(text):\n",
    "        \"\"\"HTML 태그 제거\"\"\"\n",
    "        if pd.isna(text):  # NaN 값이 있는 경우 빈 문자열로 대체\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text().strip()  # HTML 태그 제거 후 공백 제거\n",
    "\n",
    "    # 'title'에서 HTML 태그 제거 (link는 HTML 태그 없음)\n",
    "    df['title'] = df['title'].apply(clean_html)\n",
    "\n",
    "    # CSV 저장 경로 설정\n",
    "    save_dir = \"../data/csv\"\n",
    "    csv_file_name = f\"naver_{site}.csv\"\n",
    "    save_path = os.path.join(save_dir, csv_file_name)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"데이터가 정제되어 저장되었습니다: {save_path}\")\n",
    "    \n",
    "    return df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unreal_ai_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
